{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "We do it in python, because there is a much better support for stuff like multilingual bert models from huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_filename = \"settles.acl16.learning_traces.13m.csv\"  # Change to the name of your input CSV file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_lex = \"\"\n",
    "\n",
    "with open(input_filename, 'r') as csv_file:\n",
    "    reader = csv.reader(csv_file)\n",
    "    header = next(reader)  # Read the header\n",
    "    vocabulary_lex = set([(column[7], column[4], column[6]) for column in reader])\n",
    "    del header, reader\n",
    "del csv_file\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'@prn:celui_la', '@future_perfect', '@common_phrases:de_rien', '@pr:pres_de', 'adv', '@ij:bis_bald', '@common_phrases:a_plus_tard', '@modal', 'dat', 'def', '*numb', '@itg:est_ce_que', 'det', 'acr', 'p1', 'comp', 'vaux', '@cnj:autant_que', '@ij:buenas_noches', 'pro', 'pr+il', '*gndr', 'pst', '@adv:a_part', '@cnj:parce_que', '@common_phrases:a_demain', '@cnj:du_fait_que', '@neg:plus_de', 'vblex', 'n+versicherung', 'inf+ci', '@neg:pas_du_tout', 'sp', '@pluperfect', 'past', '@pr:a_travers', '@ij:thank_you', 'predet', 'num', 'pl+lo', 'vbmod', '@ij:au_revoir', '@pr:plus_de', '@subjunctive_perfect', '@pr:afin_de', 'loc', '@pr:a_cote_de', '@future', '@prn:n_importe_quoi', '@pos', 'n+essen', 'pr+ele', '@prn:le_notre', '*case', '@pr:un_peu_de', 'pprep', '@cnj:bien_que', 'adj+haltung', 'pri', 'pii', '@adv:tout_a_fait', '@adv:en_fait', 'vblex+bad', 'pron', 'sg+mi', 'pres', '@prn:ce_dont', 'sg', 'ord', '@present_perfect', 'nom', '@det:de_le', 'pr+esse', 'pl', '@future_phrasal', 'n+sandwich', 'n+meister', 'p3', 'ifi', 'cni', 'sw', 'qnt', 'nn', 'pl+ci', '@common_phrases:a_plus', 'cnjcoo', 'n+hof', '@ref', '@n:petit_ami', 'ind', 'pprs', 'prs', 'attr', '@compound_past', 'suff', '@adv:au_dela', 'n+nummer', 'p2', '@adv:s_il_te_plait', 'acc', 'ij', 'an', 'np', '@ij:auf_wiedersehen', '@prn:celui_que', '@past_inf', 'pis', 'pr', 'n+ende', 'nom.', '@passive', 'gen', 'mix', '@common_phrases:comment_ca_va', '@cnj:avant_que', 'rel', '@neg:il_ne_y_a', '@common_phrases:ca_va_bien', '@cnj:tandis_que', 'dim', 'n+kalender', '@ger_past', 'fti', '*pers', 'sup', 'nt', 'pl+gli', '@adv:a_peu_pres', 'mf', 'inf', '@prn:celui_ci', '@cnj:pour_que', 'obj', 'inf+lo', 'pr+isso', 'adj', '@adv:s_il_vous_plait', 'n+stier', 'pr+der', '@subjunctive_pluperfect', 'pos', 'pr+le', 'n+wehr', '@formal', 'subj', 'itg', '@prn:quelque_chose', '@cnj:des_que', '@adv:a_priori', 'f', 'enc', '@adv:por_favor', 'preadv', 'pr+o', '@cnj:depuis_que', 'dem', 'ant', 'n', '@pr:autant_de', '@past', '@past_cond', '@pr:a_cause_de', 'sg+lo', 'n+welt', '@cnj:pendant_que', '@cond_perfect', 'vblex+ort', '@obj', 'prn', '@pr:au_dela_de', '@cond', 'ref', '@past_perfect', 'cnjadv', 'vbdo', '@adv:en_general', 'cnjsub', 'apos', '@ij:buenos_dias', '@prn:quelque_un', 'pp', '@adv:por_supuesto', 'pred', '@det:a_le', '@cnj:alors_que', '@prn:le_tien', 'm', '@common_phrases:a_bientot', '@cnj:apres_que', '@past_subjunctive', 'vbhaver', 'pr+el', 'tn', 'sint', '@common_phrases:il_y_a', 'pr+das', '@itg:que_est_ce_que', 'vbser', 'pres+not', '@prn:le_mien', '@ij:merci_beaucoup', 'aa', '@adv:au_moins', 'st', '@adv:a_posteriori', '@prn:le_meme', '@adv:peut_etre', 'imp', 'ger', '@prn:l_un'}\n",
      "{'mf': 1123, 'n': 9033, 'm': 4177, '*numb': 5567, 'vblex': 5295, 'pri': 1976, 'p3': 1208, 'pl': 2350, 'inf': 1382, 'f': 3821, '*gndr': 1077, 'adj': 2101, 'sg': 7359, '*pers': 1103, 'nom': 1023}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['mf', 'n', 'm', '*numb', 'vblex', 'pri', 'p3', 'pl', 'inf', 'f', '*gndr', 'adj', 'sg', '*pers', 'nom'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_lex_tags = set()\n",
    "all_lex_tags_dict = {}\n",
    "\n",
    "for lex in vocabulary_lex:\n",
    "    for l in lex[0].split(\"/\")[-1].split(\"<\")[1:]:\n",
    "        lex_str = l.replace(\">\", \"\")\n",
    "        all_lex_tags.add(lex_str)\n",
    "        if lex_str in all_lex_tags_dict.keys():\n",
    "            all_lex_tags_dict[lex_str] += 1\n",
    "        else: all_lex_tags_dict[lex_str] = 1\n",
    "\n",
    "\n",
    "print(all_lex_tags)\n",
    "\n",
    "most_all_lex_tags_dict = {}\n",
    "\n",
    "for key, value in all_lex_tags_dict.items():\n",
    "    if value > 1000:\n",
    "        most_all_lex_tags_dict[key] = value\n",
    "\n",
    "print(most_all_lex_tags_dict)\n",
    "most_all_lex_tags_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do the Bert Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11198"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r'([^<]+/\\w+)<'\n",
    "\n",
    "vocabulary = set()\n",
    "for i in [re.match(pattern, s[0]).group(1) for s in vocabulary_lex if re.match(pattern, s[0])]:\n",
    "    words = i.split(\"/\")\n",
    "    vocabulary.add(words[0])\n",
    "    vocabulary.add(words[1])\n",
    "vocabulary = list(vocabulary)\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained multilingual BERT model\n",
    "model = \"google-bert/bert-base-cased\"\n",
    "# model = \"bert-base-multilingual-cased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "model = AutoModel.from_pretrained(model)\n",
    "\n",
    "\n",
    "def get_embeddings_iteratively(phrases, batch_size=2, output_file='embeddings.npy'):\n",
    "    all_embeddings = []\n",
    "    for i in range(0, len(phrases), batch_size):\n",
    "        batch = phrases[i:i + batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True)\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)  # Mean pooling\n",
    "        embeddings = embeddings.detach().numpy()\n",
    "        all_embeddings.append(embeddings)\n",
    "        del inputs, outputs, embeddings\n",
    "        torch.cuda.empty_cache()  # Clear GPU memory if using GPU\n",
    "    all_embeddings = np.vstack(all_embeddings)\n",
    "    np.save(output_file, all_embeddings)\n",
    "    return all_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_embeddings_iteratively(vocabulary, batch_size=20, output_file='embeddings.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11198, 768)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = np.load('embeddings.npy')\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensionality of embeddings\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "# plt.figure(figsize=(15, 15))\n",
    "# for i, phrase in enumerate(vocabulary):\n",
    "#     plt.scatter(reduced_embeddings[i, 0], reduced_embeddings[i, 1], label=phrase, s=3)\n",
    "#     plt.text(reduced_embeddings[i, 0], reduced_embeddings[i, 1], phrase, fontsize=7, alpha=0.4, ha='right', va='center')\n",
    "# plt.title(\"BERT Embeddings in 2D\")\n",
    "# plt.xlabel(\"PCA Component 1\")\n",
    "# plt.ylabel(\"PCA Component 2\")\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the calculated PCA embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_dataset = []\n",
    "for i, word in enumerate(vocabulary):\n",
    "    for sample in vocabulary_lex:\n",
    "        if word in sample[0]:\n",
    "            lexeme = sample[0]\n",
    "            language = sample[1]\n",
    "            idx = sample[2]\n",
    "\n",
    "    vocabulary_dataset.append({\n",
    "        \"custom_id\" : i,\n",
    "        \"word\": word,\n",
    "        \"position\": list(reduced_embeddings[i]),\n",
    "        \"language\": language,\n",
    "        \"lexeme_id\": idx,\n",
    "        \"lexeme\": lexeme\n",
    "    })\n",
    "\n",
    "\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, np.float32):\n",
    "        return float(obj)  # Convert to Python float\n",
    "    else:\n",
    "        raise TypeError(\"Type not serializable\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"../src/data/vocabulary_dataset.json\", \"w\") as f:\n",
    "    json.dump(vocabulary_dataset, f, default=convert_to_serializable, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
